{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0f8910-b47c-4408-8506-f71939ee8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c62480-422b-47e0-905f-471f80e3a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33423040-ba82-4d89-ba16-b7ba21eda9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = \"/Users/fcccasa/jupyter_project\"\n",
    "device = torch.device(\"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9761809b-57ed-46f2-a94e-b4d090e57e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "rmb_label = {\"1\": 0, \"100\": 1}\n",
    "\n",
    "class CatDogDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode=\"train\", split_n=0.9, rng_seed=620, transform=None):\n",
    "        \"\"\"\n",
    "        rmb面额分类任务的Dataset\n",
    "        :param data_dir: str, 数据集所在路径\n",
    "        :param transform: torch.transform，数据预处理\n",
    "        \"\"\"\n",
    "        self.mode = mode              # 模式设置，默认是“train”，即训练模式\n",
    "        self.data_dir = data_dir      # 数据集所在的路径\n",
    "        self.rng_seed = rng_seed      # 随机数种子，确保数据集划分的一致性\n",
    "        \"\"\"\n",
    "            设置了随机数种子，例如 random.seed(self.rng_seed)，那么每次执行 random.shuffle 时，\n",
    "            shuffle 会按照固定的方式打乱数据，使得每次划分出的训练集和验证集都是一样的，从而 保证数据集划分的一致性。\n",
    "        \"\"\"\n",
    "        self.split_n = split_n        # 训练集与验证集的比例，默认90%训练，10%验证\n",
    "        self.data_info = self._get_img_info()  # data_info存储所有图片路径和标签，在DataLoader中通过index读取样本\n",
    "        self.transform = transform    # # 用于数据预处理的transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path_img, label = self.data_info[index]       # 根据index从data_info中获取图片路径和标签\n",
    "        img = Image.open(path_img).convert('RGB')     # 0~255 # 打开图片并转化为RGB模式（确保每张图片是3通道)\n",
    "\n",
    "        if self.transform is not None:  # 如果有transform操作，应用于图片\n",
    "            img = self.transform(img)   # 将图片转换为tensor等形式，进行预处理\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.data_info) == 0:\n",
    "            raise Exception(\"\\ndata_dir:{} is a empty dir! Please checkout your path to images!\".format(self.data_dir))\n",
    "        return len(self.data_info)  # 回数据集中样本的数量\n",
    "\n",
    "    def _get_img_info(self):\n",
    "        # 获取图片信息\n",
    "        img_names = os.listdir(self.data_dir)  # 获取数据集目录下所有文件的文件名\n",
    "        img_names = list(filter(lambda x: x.endswith('.jpg'), img_names))  # 只保留.jpg文件\n",
    "\n",
    "        random.seed(self.rng_seed) # 设置随机种子，保证每次划分一致\n",
    "        random.shuffle(img_names)  # 随机打乱图片文件名顺序\n",
    "\n",
    "        img_labels = [0 if n.startswith('cat') else 1 for n in img_names] # 根据文件名判断类别（'cat' -> 0，其他 -> 1）\n",
    "\n",
    "        split_idx = int(len(img_labels) * self.split_n)  # 25000* 0.9 = 22500 # 根据split_n计算训练集和验证集的分割位置\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            img_set = img_names[:split_idx]   # 选择前90%的图片作为训练集\n",
    "            label_set = img_labels[:split_idx]\n",
    "        elif self.mode == \"valid\":\n",
    "            img_set = img_names[split_idx:]\n",
    "            label_set = img_labels[split_idx:]\n",
    "        else:\n",
    "            raise Exception(\"self.mode 无法识别，仅支持(train, valid)\")\n",
    "\n",
    "        path_img_set = [os.path.join(self.data_dir, n) for n in img_set] # 获取每个图片的完整路径\n",
    "        data_info = [(n, l) for n, l in zip(path_img_set, label_set)]    # 创建包含图片路径和标签的元组列表\n",
    "\n",
    "        return data_info   # 返回包含所有图片路径和标签的元组列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc267ff-643a-44fa-8d71-c32036eae635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度卷积 一个卷积核对应一个输入通道，分组卷积\n",
    "class dw_conv(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,stride):\n",
    "        super().__init__()\n",
    "        # 卷积操作 groups很重要 我们要吧卷积核分成多少组 这样就能一个卷积和对应一个通道了 输入输出维度指的是通道\n",
    "        self.dw_conv_k3 = nn.Conv2d(in_dim,out_dim,kernel_size = 3,stride = stride,groups = in_dim,bias = False)\n",
    "        self.bn = nn.BatchNorm2d(out_dim)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.dw_conv_k3(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "453f34de-65d6-43ed-b939-6674286296e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点卷积\n",
    "class point_conv(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super().__init__()\n",
    "        # 卷积操作 groups很重要 我们要吧卷积核分成多少组 这样就能一个卷积和对应一个通道了 输入输出维度指的是通道\n",
    "        self.p_conv_k1 = nn.Conv2d(in_dim,out_dim,kernel_size = 1,bias = False)\n",
    "        self.bn = nn.BatchNorm2d(out_dim)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.p_conv_k1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4ee9a4-538e-45bf-8f07-8c5f4954219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNets(nn.Module):\n",
    "    def __init__(self,num_classes,large_img):\n",
    "        super(MobileNets,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        if large_img:\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3,32,kernel_size = 3,stride = 2),\n",
    "                nn.ReLU(inplace = True),\n",
    "                dw_conv(32,32, 1),\n",
    "                point_conv(32, 64),\n",
    "                dw_conv(64,64, 2),\n",
    "                point_conv(64, 128),\n",
    "                dw_conv(128, 128, 1),\n",
    "                point_conv(128, 128),\n",
    "                dw_conv(128, 128, 2),\n",
    "                point_conv(128, 256),\n",
    "                dw_conv(256, 256, 1),\n",
    "                point_conv(256, 256),\n",
    "                dw_conv(256, 256, 2),\n",
    "                point_conv(256, 512),\n",
    "                dw_conv(512, 512, 1),\n",
    "                point_conv(512, 512),\n",
    "                dw_conv(512, 512, 1),\n",
    "                point_conv(512, 512),\n",
    "                dw_conv(512, 512, 1),\n",
    "                point_conv(512, 512),\n",
    "                dw_conv(512, 512, 1),\n",
    "                point_conv(512, 512),\n",
    "                dw_conv(512, 512, 1),\n",
    "                point_conv(512, 512),\n",
    "                dw_conv(512, 512, 2),\n",
    "                point_conv(512, 1024),\n",
    "                dw_conv(1024, 1024, 2),\n",
    "                point_conv(1024, 1024),\n",
    "                nn.AdaptiveAvgPool2d(1),  # 使用适应池化,\n",
    "            )\n",
    "        else:\n",
    "            # 唯一区别步长全是1\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3,32,kernel_size = 3,stride = 1),\n",
    "                nn.ReLU(inplace = True),\n",
    "                dw_conv(32,32, 1),\n",
    "                point_conv(32, 64),\n",
    "                dw_conv(64,64, 1),\n",
    "                point_conv(64, 128),\n",
    "                dw_conv(128, 128, 1),\n",
    "                point_conv(128, 128),\n",
    "                dw_conv(128, 128, 1),\n",
    "                point_conv(128, 256),\n",
    "                dw_conv(256, 256, 1),\n",
    "                point_conv(256, 256),\n",
    "                dw_conv(256, 256, 1),\n",
    "                point_conv(256, 512),\n",
    "                dw_conv(512, 512, 1),\n",
    "                point_conv(512, 512),\n",
    "                dw_conv(512, 512, 1),\n",
    "                point_conv(512, 512),\n",
    "                dw_conv(512, 512, 1),\n",
    "                point_conv(512, 512),\n",
    "                dw_conv(512, 512, 1),\n",
    "                point_conv(512, 512),\n",
    "                dw_conv(512, 512, 1),\n",
    "                point_conv(512, 512),\n",
    "                dw_conv(512, 512, 1),\n",
    "                point_conv(512, 1024),\n",
    "                dw_conv(1024, 1024, 1),\n",
    "                point_conv(1024, 1024),\n",
    "                nn.AdaptiveAvgPool2d(1),  # 使用适应池化,\n",
    "            )\n",
    "        # num_classes 分类数\n",
    "        self.fc = nn.Linear(1024,self.num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1,1024)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07bd612e-a0ba-4a6b-875c-42b2bea4bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "def get_model(vis_model=False):\n",
    "    \"\"\"\n",
    "    创建模型，加载参数\n",
    "    :param path_state_dict:是一个字符串，表示预训练模型的权重文件路径（即模型的状态字典）。\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model = MobileNets(10,False) # 创建一个 AlexNet 模型实例\n",
    "\n",
    "    if vis_model:\n",
    "        from torchsummary import summary\n",
    "        summary(model, input_size=(3, 224, 224), device=\"cpu\")\n",
    "        \"\"\"\n",
    "            summary 可以输出模型的层次结构、每一层的参数量、输出的形状等信息。\n",
    "            导入 torchsummary 库中的 summary 函数。torchsummary 是一个用于显示 PyTorch 模型结构和参数信息的库。\n",
    "            model：表示模型对象，在这里是 AlexNet 模型。\n",
    "            \n",
    "            input_size=(3, 224, 224)：定义输入张量的尺寸，表示输入图像的通道数（3，RGB图像）和图像的尺寸（224x224像素）。\n",
    "            对于 AlexNet 来说，输入图像通常是 224x224 的 RGB 图像。\n",
    "        \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    return model # 返回创建并加载了预训练权重的模型对象 model。这个模型将可以用于推理（预测）或者进一步的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2148fe7-025b-4914-9817-4661c6c2a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化\n",
    "data_dir = os.path.join(BASE_DIR, \"cifar10\",\"cifar-10-batches-py\")\n",
    "num_classes = 2\n",
    "\n",
    "MAX_EPOCH = 3       \n",
    "BATCH_SIZE = 128  # 批量大小与显存容量和数据集大小密切相关。较大的 BATCH_SIZE 可以加快训练速度，但需要更大的显存。一次性处理 256 个样本。\n",
    "LR = 0.001          \n",
    "log_interval = 1    # 每隔 1 个 训练批次（或 epoch）记录一次训练日志。 表示训练过程中，控制打印训练信息的频率。\n",
    "val_interval = 1    # 每隔 1 个 epoch 进行一次验证集的评估。验证集的评估是为了监控模型在验证集上的表现，从而避免过拟合。\n",
    "classes = 2         # 分类任务的类别数为 2。\n",
    "start_epoch = -1     \n",
    "lr_decay_step = 1   # 表示学习率的衰减步长。 每隔一定的 epoch（如 1）对学习率进行衰减，通常是为了使模型在后期更稳定地收敛。\n",
    "                    # 可结合优化器的学习率调度策略（如 StepLR）使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1227952f-3de1-4708-a74c-afaee15f3b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "normalizes = transforms.Normalize(norm_mean, norm_std)\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),                # 将图像缩放到 256x256 大小。\n",
    "    transforms.TenCrop(30, vertical_flip=False), # 从图像中裁剪 10 个 224x224 的区域，包括 4 个角和 1 个中心，以及它们的水平翻转版本。\n",
    "    transforms.Lambda(lambda crops: torch.stack([normalizes(transforms.ToTensor()(crop)) for crop in crops])),\n",
    "])\n",
    "\n",
    "# 构建MyDataset实例 \n",
    "train_data = datasets.CIFAR10(root=\"cifar10\", train=True, download=False, transform=train_transform)\n",
    "valid_data = datasets.CIFAR10(root=\"cifar10\", train=False, download=False,transform=valid_transform)\n",
    "\n",
    "# 构建DataLoder\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=32) # 验证集通常不需要打乱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3e79e5c-25fa-46b1-81cd-a4622a200fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNets(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): point_conv(\n",
       "      (p_conv_k1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), groups=64, bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): point_conv(\n",
       "      (p_conv_k1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): point_conv(\n",
       "      (p_conv_k1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): point_conv(\n",
       "      (p_conv_k1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256, bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): point_conv(\n",
       "      (p_conv_k1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256, bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): point_conv(\n",
       "      (p_conv_k1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): point_conv(\n",
       "      (p_conv_k1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): point_conv(\n",
       "      (p_conv_k1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): point_conv(\n",
       "      (p_conv_k1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): point_conv(\n",
       "      (p_conv_k1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (23): point_conv(\n",
       "      (p_conv_k1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (24): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (25): point_conv(\n",
       "      (p_conv_k1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (26): dw_conv(\n",
       "      (dw_conv_k3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), groups=1024, bias=False)\n",
       "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (27): point_conv(\n",
       "      (p_conv_k1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (28): AdaptiveAvgPool2d(output_size=1)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型\n",
    "Mobile_model = get_model(False)\n",
    "\n",
    "Mobile_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64fac7d1-af71-4375-9036-4faf0b4a7189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdff5f53-3e43-4839-94d6-4fe2c57fb5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(Mobile_model.parameters(), lr=LR, momentum=0.9)  # 选择优化器\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=0.1)  # 设置学习率下降策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e0db2-7d28-4706-85bf-0a5de614993e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:Epoch[000/003] Iteration[001/391] Loss: 2.3497 Acc:10.94%\n",
      "Training:Epoch[000/003] Iteration[002/391] Loss: 2.3552 Acc:8.59%\n",
      "Training:Epoch[000/003] Iteration[003/391] Loss: 2.3360 Acc:9.38%\n",
      "Training:Epoch[000/003] Iteration[004/391] Loss: 2.3380 Acc:9.18%\n",
      "Training:Epoch[000/003] Iteration[005/391] Loss: 2.3394 Acc:9.38%\n",
      "Training:Epoch[000/003] Iteration[006/391] Loss: 2.3118 Acc:9.90%\n",
      "Training:Epoch[000/003] Iteration[007/391] Loss: 2.2991 Acc:9.71%\n",
      "Training:Epoch[000/003] Iteration[008/391] Loss: 2.3146 Acc:9.77%\n",
      "Training:Epoch[000/003] Iteration[009/391] Loss: 2.3247 Acc:9.64%\n",
      "Training:Epoch[000/003] Iteration[010/391] Loss: 2.2963 Acc:10.00%\n",
      "Training:Epoch[000/003] Iteration[011/391] Loss: 2.3186 Acc:10.01%\n",
      "Training:Epoch[000/003] Iteration[012/391] Loss: 2.2945 Acc:9.96%\n",
      "Training:Epoch[000/003] Iteration[013/391] Loss: 2.2918 Acc:9.98%\n",
      "Training:Epoch[000/003] Iteration[014/391] Loss: 2.3022 Acc:9.88%\n",
      "Training:Epoch[000/003] Iteration[015/391] Loss: 2.3095 Acc:9.95%\n",
      "Training:Epoch[000/003] Iteration[016/391] Loss: 2.3101 Acc:10.01%\n",
      "Training:Epoch[000/003] Iteration[017/391] Loss: 2.3066 Acc:10.16%\n",
      "Training:Epoch[000/003] Iteration[018/391] Loss: 2.3096 Acc:10.03%\n",
      "Training:Epoch[000/003] Iteration[019/391] Loss: 2.2947 Acc:10.07%\n",
      "Training:Epoch[000/003] Iteration[020/391] Loss: 2.2912 Acc:10.23%\n",
      "Training:Epoch[000/003] Iteration[021/391] Loss: 2.2588 Acc:10.71%\n",
      "Training:Epoch[000/003] Iteration[022/391] Loss: 2.2982 Acc:10.58%\n",
      "Training:Epoch[000/003] Iteration[023/391] Loss: 2.3029 Acc:10.39%\n",
      "Training:Epoch[000/003] Iteration[024/391] Loss: 2.2632 Acc:10.74%\n",
      "Training:Epoch[000/003] Iteration[025/391] Loss: 2.2927 Acc:10.97%\n",
      "Training:Epoch[000/003] Iteration[026/391] Loss: 2.2765 Acc:11.03%\n",
      "Training:Epoch[000/003] Iteration[027/391] Loss: 2.3169 Acc:11.02%\n",
      "Training:Epoch[000/003] Iteration[028/391] Loss: 2.3086 Acc:10.99%\n",
      "Training:Epoch[000/003] Iteration[029/391] Loss: 2.3150 Acc:10.96%\n",
      "Training:Epoch[000/003] Iteration[030/391] Loss: 2.2954 Acc:11.09%\n",
      "Training:Epoch[000/003] Iteration[031/391] Loss: 2.2965 Acc:11.09%\n",
      "Training:Epoch[000/003] Iteration[032/391] Loss: 2.2889 Acc:11.11%\n",
      "Training:Epoch[000/003] Iteration[033/391] Loss: 2.3156 Acc:11.10%\n",
      "Training:Epoch[000/003] Iteration[034/391] Loss: 2.2945 Acc:11.17%\n",
      "Training:Epoch[000/003] Iteration[035/391] Loss: 2.2927 Acc:11.18%\n",
      "Training:Epoch[000/003] Iteration[036/391] Loss: 2.3132 Acc:11.15%\n",
      "Training:Epoch[000/003] Iteration[037/391] Loss: 2.3076 Acc:11.15%\n",
      "Training:Epoch[000/003] Iteration[038/391] Loss: 2.2884 Acc:11.14%\n",
      "Training:Epoch[000/003] Iteration[039/391] Loss: 2.2949 Acc:11.08%\n",
      "Training:Epoch[000/003] Iteration[040/391] Loss: 2.2520 Acc:11.19%\n",
      "Training:Epoch[000/003] Iteration[041/391] Loss: 2.3023 Acc:11.09%\n",
      "Training:Epoch[000/003] Iteration[042/391] Loss: 2.2980 Acc:11.10%\n",
      "Training:Epoch[000/003] Iteration[043/391] Loss: 2.2613 Acc:11.12%\n",
      "Training:Epoch[000/003] Iteration[044/391] Loss: 2.3218 Acc:11.08%\n",
      "Training:Epoch[000/003] Iteration[045/391] Loss: 2.2575 Acc:11.28%\n",
      "Training:Epoch[000/003] Iteration[046/391] Loss: 2.3022 Acc:11.29%\n",
      "Training:Epoch[000/003] Iteration[047/391] Loss: 2.2856 Acc:11.32%\n",
      "Training:Epoch[000/003] Iteration[048/391] Loss: 2.2459 Acc:11.43%\n",
      "Training:Epoch[000/003] Iteration[049/391] Loss: 2.3050 Acc:11.35%\n",
      "Training:Epoch[000/003] Iteration[050/391] Loss: 2.2821 Acc:11.41%\n",
      "Training:Epoch[000/003] Iteration[051/391] Loss: 2.2794 Acc:11.43%\n",
      "Training:Epoch[000/003] Iteration[052/391] Loss: 2.2769 Acc:11.43%\n",
      "Training:Epoch[000/003] Iteration[053/391] Loss: 2.2896 Acc:11.39%\n",
      "Training:Epoch[000/003] Iteration[054/391] Loss: 2.2414 Acc:11.49%\n",
      "Training:Epoch[000/003] Iteration[055/391] Loss: 2.2835 Acc:11.55%\n",
      "Training:Epoch[000/003] Iteration[056/391] Loss: 2.3167 Acc:11.61%\n",
      "Training:Epoch[000/003] Iteration[057/391] Loss: 2.2981 Acc:11.64%\n",
      "Training:Epoch[000/003] Iteration[058/391] Loss: 2.3090 Acc:11.53%\n",
      "Training:Epoch[000/003] Iteration[059/391] Loss: 2.2864 Acc:11.60%\n",
      "Training:Epoch[000/003] Iteration[060/391] Loss: 2.2728 Acc:11.69%\n",
      "Training:Epoch[000/003] Iteration[061/391] Loss: 2.2738 Acc:11.72%\n",
      "Training:Epoch[000/003] Iteration[062/391] Loss: 2.3051 Acc:11.69%\n",
      "Training:Epoch[000/003] Iteration[063/391] Loss: 2.2855 Acc:11.71%\n",
      "Training:Epoch[000/003] Iteration[064/391] Loss: 2.3020 Acc:11.74%\n",
      "Training:Epoch[000/003] Iteration[065/391] Loss: 2.2850 Acc:11.77%\n",
      "Training:Epoch[000/003] Iteration[066/391] Loss: 2.2932 Acc:11.70%\n",
      "Training:Epoch[000/003] Iteration[067/391] Loss: 2.2415 Acc:11.79%\n",
      "Training:Epoch[000/003] Iteration[068/391] Loss: 2.2733 Acc:11.88%\n",
      "Training:Epoch[000/003] Iteration[069/391] Loss: 2.2795 Acc:11.88%\n",
      "Training:Epoch[000/003] Iteration[070/391] Loss: 2.3155 Acc:11.88%\n",
      "Training:Epoch[000/003] Iteration[071/391] Loss: 2.2542 Acc:11.97%\n",
      "Training:Epoch[000/003] Iteration[072/391] Loss: 2.2851 Acc:11.96%\n",
      "Training:Epoch[000/003] Iteration[073/391] Loss: 2.3140 Acc:11.92%\n",
      "Training:Epoch[000/003] Iteration[074/391] Loss: 2.2313 Acc:11.94%\n",
      "Training:Epoch[000/003] Iteration[075/391] Loss: 2.2668 Acc:12.02%\n",
      "Training:Epoch[000/003] Iteration[076/391] Loss: 2.3005 Acc:11.96%\n",
      "Training:Epoch[000/003] Iteration[077/391] Loss: 2.2671 Acc:11.95%\n",
      "Training:Epoch[000/003] Iteration[078/391] Loss: 2.2900 Acc:11.98%\n",
      "Training:Epoch[000/003] Iteration[079/391] Loss: 2.2735 Acc:12.00%\n",
      "Training:Epoch[000/003] Iteration[080/391] Loss: 2.3064 Acc:11.98%\n",
      "Training:Epoch[000/003] Iteration[081/391] Loss: 2.2273 Acc:12.02%\n",
      "Training:Epoch[000/003] Iteration[082/391] Loss: 2.2786 Acc:12.00%\n",
      "Training:Epoch[000/003] Iteration[083/391] Loss: 2.2233 Acc:12.07%\n",
      "Training:Epoch[000/003] Iteration[084/391] Loss: 2.2761 Acc:12.05%\n",
      "Training:Epoch[000/003] Iteration[085/391] Loss: 2.2927 Acc:12.10%\n",
      "Training:Epoch[000/003] Iteration[086/391] Loss: 2.2574 Acc:12.15%\n",
      "Training:Epoch[000/003] Iteration[087/391] Loss: 2.2714 Acc:12.15%\n",
      "Training:Epoch[000/003] Iteration[088/391] Loss: 2.2297 Acc:12.21%\n",
      "Training:Epoch[000/003] Iteration[089/391] Loss: 2.2957 Acc:12.14%\n",
      "Training:Epoch[000/003] Iteration[090/391] Loss: 2.2499 Acc:12.16%\n",
      "Training:Epoch[000/003] Iteration[091/391] Loss: 2.2884 Acc:12.17%\n",
      "Training:Epoch[000/003] Iteration[092/391] Loss: 2.2950 Acc:12.16%\n",
      "Training:Epoch[000/003] Iteration[093/391] Loss: 2.3044 Acc:12.14%\n",
      "Training:Epoch[000/003] Iteration[094/391] Loss: 2.3007 Acc:12.13%\n",
      "Training:Epoch[000/003] Iteration[095/391] Loss: 2.2708 Acc:12.17%\n",
      "Training:Epoch[000/003] Iteration[096/391] Loss: 2.2580 Acc:12.21%\n",
      "Training:Epoch[000/003] Iteration[097/391] Loss: 2.2880 Acc:12.17%\n",
      "Training:Epoch[000/003] Iteration[098/391] Loss: 2.2674 Acc:12.18%\n",
      "Training:Epoch[000/003] Iteration[099/391] Loss: 2.2734 Acc:12.15%\n",
      "Training:Epoch[000/003] Iteration[100/391] Loss: 2.2756 Acc:12.14%\n",
      "Training:Epoch[000/003] Iteration[101/391] Loss: 2.2274 Acc:12.18%\n",
      "Training:Epoch[000/003] Iteration[102/391] Loss: 2.2706 Acc:12.25%\n",
      "Training:Epoch[000/003] Iteration[103/391] Loss: 2.2524 Acc:12.29%\n",
      "Training:Epoch[000/003] Iteration[104/391] Loss: 2.2512 Acc:12.32%\n",
      "Training:Epoch[000/003] Iteration[105/391] Loss: 2.2421 Acc:12.40%\n",
      "Training:Epoch[000/003] Iteration[106/391] Loss: 2.2489 Acc:12.46%\n",
      "Training:Epoch[000/003] Iteration[107/391] Loss: 2.2661 Acc:12.47%\n",
      "Training:Epoch[000/003] Iteration[108/391] Loss: 2.1950 Acc:12.54%\n",
      "Training:Epoch[000/003] Iteration[109/391] Loss: 2.2973 Acc:12.55%\n",
      "Training:Epoch[000/003] Iteration[110/391] Loss: 2.2677 Acc:12.56%\n",
      "Training:Epoch[000/003] Iteration[111/391] Loss: 2.2818 Acc:12.56%\n",
      "Training:Epoch[000/003] Iteration[112/391] Loss: 2.2867 Acc:12.51%\n",
      "Training:Epoch[000/003] Iteration[113/391] Loss: 2.2612 Acc:12.52%\n",
      "Training:Epoch[000/003] Iteration[114/391] Loss: 2.3286 Acc:12.48%\n",
      "Training:Epoch[000/003] Iteration[115/391] Loss: 2.2346 Acc:12.46%\n",
      "Training:Epoch[000/003] Iteration[116/391] Loss: 2.2647 Acc:12.47%\n",
      "Training:Epoch[000/003] Iteration[117/391] Loss: 2.2730 Acc:12.46%\n",
      "Training:Epoch[000/003] Iteration[118/391] Loss: 2.2165 Acc:12.50%\n",
      "Training:Epoch[000/003] Iteration[119/391] Loss: 2.2998 Acc:12.49%\n",
      "Training:Epoch[000/003] Iteration[120/391] Loss: 2.3087 Acc:12.47%\n",
      "Training:Epoch[000/003] Iteration[121/391] Loss: 2.2533 Acc:12.49%\n",
      "Training:Epoch[000/003] Iteration[122/391] Loss: 2.2897 Acc:12.49%\n",
      "Training:Epoch[000/003] Iteration[123/391] Loss: 2.2635 Acc:12.49%\n",
      "Training:Epoch[000/003] Iteration[124/391] Loss: 2.2470 Acc:12.49%\n",
      "Training:Epoch[000/003] Iteration[125/391] Loss: 2.2143 Acc:12.53%\n",
      "Training:Epoch[000/003] Iteration[126/391] Loss: 2.2268 Acc:12.53%\n",
      "Training:Epoch[000/003] Iteration[127/391] Loss: 2.2804 Acc:12.56%\n",
      "Training:Epoch[000/003] Iteration[128/391] Loss: 2.2685 Acc:12.60%\n",
      "Training:Epoch[000/003] Iteration[129/391] Loss: 2.2766 Acc:12.60%\n",
      "Training:Epoch[000/003] Iteration[130/391] Loss: 2.2761 Acc:12.60%\n",
      "Training:Epoch[000/003] Iteration[131/391] Loss: 2.2601 Acc:12.63%\n",
      "Training:Epoch[000/003] Iteration[132/391] Loss: 2.2476 Acc:12.64%\n",
      "Training:Epoch[000/003] Iteration[133/391] Loss: 2.2688 Acc:12.64%\n",
      "Training:Epoch[000/003] Iteration[134/391] Loss: 2.2360 Acc:12.66%\n",
      "Training:Epoch[000/003] Iteration[135/391] Loss: 2.2365 Acc:12.70%\n",
      "Training:Epoch[000/003] Iteration[136/391] Loss: 2.2398 Acc:12.71%\n",
      "Training:Epoch[000/003] Iteration[137/391] Loss: 2.2235 Acc:12.69%\n",
      "Training:Epoch[000/003] Iteration[138/391] Loss: 2.2540 Acc:12.72%\n",
      "Training:Epoch[000/003] Iteration[139/391] Loss: 2.2456 Acc:12.69%\n",
      "Training:Epoch[000/003] Iteration[140/391] Loss: 2.2562 Acc:12.68%\n",
      "Training:Epoch[000/003] Iteration[141/391] Loss: 2.2432 Acc:12.67%\n",
      "Training:Epoch[000/003] Iteration[142/391] Loss: 2.2901 Acc:12.66%\n",
      "Training:Epoch[000/003] Iteration[143/391] Loss: 2.2558 Acc:12.69%\n",
      "Training:Epoch[000/003] Iteration[144/391] Loss: 2.2297 Acc:12.75%\n",
      "Training:Epoch[000/003] Iteration[145/391] Loss: 2.2316 Acc:12.76%\n",
      "Training:Epoch[000/003] Iteration[146/391] Loss: 2.2439 Acc:12.79%\n",
      "Training:Epoch[000/003] Iteration[147/391] Loss: 2.2493 Acc:12.80%\n",
      "Training:Epoch[000/003] Iteration[148/391] Loss: 2.2272 Acc:12.81%\n",
      "Training:Epoch[000/003] Iteration[149/391] Loss: 2.2387 Acc:12.81%\n",
      "Training:Epoch[000/003] Iteration[150/391] Loss: 2.2334 Acc:12.84%\n",
      "Training:Epoch[000/003] Iteration[151/391] Loss: 2.2581 Acc:12.83%\n",
      "Training:Epoch[000/003] Iteration[152/391] Loss: 2.2483 Acc:12.85%\n",
      "Training:Epoch[000/003] Iteration[153/391] Loss: 2.2300 Acc:12.85%\n",
      "Training:Epoch[000/003] Iteration[154/391] Loss: 2.2292 Acc:12.89%\n",
      "Training:Epoch[000/003] Iteration[155/391] Loss: 2.2741 Acc:12.89%\n",
      "Training:Epoch[000/003] Iteration[156/391] Loss: 2.2210 Acc:12.88%\n",
      "Training:Epoch[000/003] Iteration[157/391] Loss: 2.2649 Acc:12.87%\n",
      "Training:Epoch[000/003] Iteration[158/391] Loss: 2.2284 Acc:12.88%\n",
      "Training:Epoch[000/003] Iteration[159/391] Loss: 2.2527 Acc:12.87%\n",
      "Training:Epoch[000/003] Iteration[160/391] Loss: 2.1961 Acc:12.92%\n",
      "Training:Epoch[000/003] Iteration[161/391] Loss: 2.2631 Acc:12.91%\n",
      "Training:Epoch[000/003] Iteration[162/391] Loss: 2.2064 Acc:12.93%\n",
      "Training:Epoch[000/003] Iteration[163/391] Loss: 2.2172 Acc:12.94%\n",
      "Training:Epoch[000/003] Iteration[164/391] Loss: 2.2398 Acc:12.93%\n",
      "Training:Epoch[000/003] Iteration[165/391] Loss: 2.2507 Acc:12.95%\n",
      "Training:Epoch[000/003] Iteration[166/391] Loss: 2.2011 Acc:12.98%\n",
      "Training:Epoch[000/003] Iteration[167/391] Loss: 2.2123 Acc:13.02%\n",
      "Training:Epoch[000/003] Iteration[168/391] Loss: 2.2303 Acc:13.03%\n",
      "Training:Epoch[000/003] Iteration[169/391] Loss: 2.2177 Acc:13.06%\n",
      "Training:Epoch[000/003] Iteration[170/391] Loss: 2.2532 Acc:13.09%\n",
      "Training:Epoch[000/003] Iteration[171/391] Loss: 2.1733 Acc:13.14%\n",
      "Training:Epoch[000/003] Iteration[172/391] Loss: 2.2273 Acc:13.15%\n",
      "Training:Epoch[000/003] Iteration[173/391] Loss: 2.2243 Acc:13.19%\n",
      "Training:Epoch[000/003] Iteration[174/391] Loss: 2.2051 Acc:13.21%\n",
      "Training:Epoch[000/003] Iteration[175/391] Loss: 2.2561 Acc:13.25%\n",
      "Training:Epoch[000/003] Iteration[176/391] Loss: 2.2292 Acc:13.25%\n",
      "Training:Epoch[000/003] Iteration[177/391] Loss: 2.2622 Acc:13.23%\n",
      "Training:Epoch[000/003] Iteration[178/391] Loss: 2.2392 Acc:13.25%\n",
      "Training:Epoch[000/003] Iteration[179/391] Loss: 2.2690 Acc:13.27%\n",
      "Training:Epoch[000/003] Iteration[180/391] Loss: 2.2440 Acc:13.25%\n",
      "Training:Epoch[000/003] Iteration[181/391] Loss: 2.2637 Acc:13.23%\n",
      "Training:Epoch[000/003] Iteration[182/391] Loss: 2.2277 Acc:13.28%\n",
      "Training:Epoch[000/003] Iteration[183/391] Loss: 2.2180 Acc:13.30%\n",
      "Training:Epoch[000/003] Iteration[184/391] Loss: 2.2456 Acc:13.29%\n",
      "Training:Epoch[000/003] Iteration[185/391] Loss: 2.2394 Acc:13.30%\n",
      "Training:Epoch[000/003] Iteration[186/391] Loss: 2.1758 Acc:13.31%\n",
      "Training:Epoch[000/003] Iteration[187/391] Loss: 2.2348 Acc:13.33%\n",
      "Training:Epoch[000/003] Iteration[188/391] Loss: 2.2686 Acc:13.32%\n",
      "Training:Epoch[000/003] Iteration[189/391] Loss: 2.1764 Acc:13.32%\n",
      "Training:Epoch[000/003] Iteration[190/391] Loss: 2.2279 Acc:13.34%\n",
      "Training:Epoch[000/003] Iteration[191/391] Loss: 2.2262 Acc:13.33%\n",
      "Training:Epoch[000/003] Iteration[192/391] Loss: 2.2297 Acc:13.32%\n",
      "Training:Epoch[000/003] Iteration[193/391] Loss: 2.2368 Acc:13.33%\n",
      "Training:Epoch[000/003] Iteration[194/391] Loss: 2.2240 Acc:13.34%\n",
      "Training:Epoch[000/003] Iteration[195/391] Loss: 2.2604 Acc:13.35%\n",
      "Training:Epoch[000/003] Iteration[196/391] Loss: 2.2589 Acc:13.35%\n",
      "Training:Epoch[000/003] Iteration[197/391] Loss: 2.2650 Acc:13.34%\n",
      "Training:Epoch[000/003] Iteration[198/391] Loss: 2.2268 Acc:13.35%\n",
      "Training:Epoch[000/003] Iteration[199/391] Loss: 2.1948 Acc:13.39%\n",
      "Training:Epoch[000/003] Iteration[200/391] Loss: 2.2041 Acc:13.39%\n",
      "Training:Epoch[000/003] Iteration[201/391] Loss: 2.2587 Acc:13.39%\n",
      "Training:Epoch[000/003] Iteration[202/391] Loss: 2.2544 Acc:13.37%\n",
      "Training:Epoch[000/003] Iteration[203/391] Loss: 2.1963 Acc:13.40%\n",
      "Training:Epoch[000/003] Iteration[204/391] Loss: 2.2555 Acc:13.40%\n",
      "Training:Epoch[000/003] Iteration[205/391] Loss: 2.2164 Acc:13.41%\n",
      "Training:Epoch[000/003] Iteration[206/391] Loss: 2.2227 Acc:13.43%\n",
      "Training:Epoch[000/003] Iteration[207/391] Loss: 2.2259 Acc:13.45%\n",
      "Training:Epoch[000/003] Iteration[208/391] Loss: 2.2061 Acc:13.45%\n",
      "Training:Epoch[000/003] Iteration[209/391] Loss: 2.2360 Acc:13.45%\n",
      "Training:Epoch[000/003] Iteration[210/391] Loss: 2.2090 Acc:13.44%\n",
      "Training:Epoch[000/003] Iteration[211/391] Loss: 2.2675 Acc:13.45%\n",
      "Training:Epoch[000/003] Iteration[212/391] Loss: 2.2746 Acc:13.45%\n",
      "Training:Epoch[000/003] Iteration[213/391] Loss: 2.2512 Acc:13.45%\n",
      "Training:Epoch[000/003] Iteration[214/391] Loss: 2.1823 Acc:13.46%\n",
      "Training:Epoch[000/003] Iteration[215/391] Loss: 2.2075 Acc:13.46%\n",
      "Training:Epoch[000/003] Iteration[216/391] Loss: 2.2245 Acc:13.47%\n",
      "Training:Epoch[000/003] Iteration[217/391] Loss: 2.2307 Acc:13.49%\n",
      "Training:Epoch[000/003] Iteration[218/391] Loss: 2.2041 Acc:13.52%\n",
      "Training:Epoch[000/003] Iteration[219/391] Loss: 2.3229 Acc:13.52%\n",
      "Training:Epoch[000/003] Iteration[220/391] Loss: 2.2479 Acc:13.50%\n",
      "Training:Epoch[000/003] Iteration[221/391] Loss: 2.2690 Acc:13.49%\n",
      "Training:Epoch[000/003] Iteration[222/391] Loss: 2.2580 Acc:13.49%\n",
      "Training:Epoch[000/003] Iteration[223/391] Loss: 2.2085 Acc:13.51%\n",
      "Training:Epoch[000/003] Iteration[224/391] Loss: 2.2408 Acc:13.54%\n",
      "Training:Epoch[000/003] Iteration[225/391] Loss: 2.2689 Acc:13.56%\n",
      "Training:Epoch[000/003] Iteration[226/391] Loss: 2.1864 Acc:13.58%\n",
      "Training:Epoch[000/003] Iteration[227/391] Loss: 2.2433 Acc:13.60%\n",
      "Training:Epoch[000/003] Iteration[228/391] Loss: 2.2304 Acc:13.60%\n",
      "Training:Epoch[000/003] Iteration[229/391] Loss: 2.1788 Acc:13.59%\n",
      "Training:Epoch[000/003] Iteration[230/391] Loss: 2.2356 Acc:13.58%\n",
      "Training:Epoch[000/003] Iteration[231/391] Loss: 2.2457 Acc:13.58%\n",
      "Training:Epoch[000/003] Iteration[232/391] Loss: 2.2302 Acc:13.60%\n",
      "Training:Epoch[000/003] Iteration[233/391] Loss: 2.1933 Acc:13.62%\n",
      "Training:Epoch[000/003] Iteration[234/391] Loss: 2.2338 Acc:13.64%\n",
      "Training:Epoch[000/003] Iteration[235/391] Loss: 2.1680 Acc:13.69%\n",
      "Training:Epoch[000/003] Iteration[236/391] Loss: 2.1893 Acc:13.70%\n",
      "Training:Epoch[000/003] Iteration[237/391] Loss: 2.1877 Acc:13.71%\n",
      "Training:Epoch[000/003] Iteration[238/391] Loss: 2.2347 Acc:13.71%\n",
      "Training:Epoch[000/003] Iteration[239/391] Loss: 2.2231 Acc:13.71%\n",
      "Training:Epoch[000/003] Iteration[240/391] Loss: 2.2405 Acc:13.72%\n",
      "Training:Epoch[000/003] Iteration[241/391] Loss: 2.2270 Acc:13.72%\n",
      "Training:Epoch[000/003] Iteration[242/391] Loss: 2.1586 Acc:13.75%\n",
      "Training:Epoch[000/003] Iteration[243/391] Loss: 2.1754 Acc:13.75%\n",
      "Training:Epoch[000/003] Iteration[244/391] Loss: 2.1906 Acc:13.74%\n",
      "Training:Epoch[000/003] Iteration[245/391] Loss: 2.2309 Acc:13.76%\n",
      "Training:Epoch[000/003] Iteration[246/391] Loss: 2.2107 Acc:13.80%\n",
      "Training:Epoch[000/003] Iteration[247/391] Loss: 2.2031 Acc:13.82%\n",
      "Training:Epoch[000/003] Iteration[248/391] Loss: 2.2279 Acc:13.83%\n",
      "Training:Epoch[000/003] Iteration[249/391] Loss: 2.1377 Acc:13.85%\n",
      "Training:Epoch[000/003] Iteration[250/391] Loss: 2.1829 Acc:13.86%\n",
      "Training:Epoch[000/003] Iteration[251/391] Loss: 2.2531 Acc:13.88%\n",
      "Training:Epoch[000/003] Iteration[252/391] Loss: 2.2412 Acc:13.90%\n",
      "Training:Epoch[000/003] Iteration[253/391] Loss: 2.1631 Acc:13.95%\n",
      "Training:Epoch[000/003] Iteration[254/391] Loss: 2.1942 Acc:13.96%\n",
      "Training:Epoch[000/003] Iteration[255/391] Loss: 2.1675 Acc:13.98%\n",
      "Training:Epoch[000/003] Iteration[256/391] Loss: 2.1624 Acc:14.00%\n",
      "Training:Epoch[000/003] Iteration[257/391] Loss: 2.1917 Acc:14.04%\n",
      "Training:Epoch[000/003] Iteration[258/391] Loss: 2.1949 Acc:14.06%\n",
      "Training:Epoch[000/003] Iteration[259/391] Loss: 2.1510 Acc:14.09%\n",
      "Training:Epoch[000/003] Iteration[260/391] Loss: 2.1477 Acc:14.10%\n",
      "Training:Epoch[000/003] Iteration[261/391] Loss: 2.2146 Acc:14.11%\n",
      "Training:Epoch[000/003] Iteration[262/391] Loss: 2.2443 Acc:14.10%\n",
      "Training:Epoch[000/003] Iteration[263/391] Loss: 2.1850 Acc:14.10%\n",
      "Training:Epoch[000/003] Iteration[264/391] Loss: 2.1489 Acc:14.11%\n",
      "Training:Epoch[000/003] Iteration[265/391] Loss: 2.1889 Acc:14.12%\n",
      "Training:Epoch[000/003] Iteration[266/391] Loss: 2.1869 Acc:14.12%\n",
      "Training:Epoch[000/003] Iteration[267/391] Loss: 2.1935 Acc:14.14%\n",
      "Training:Epoch[000/003] Iteration[268/391] Loss: 2.2084 Acc:14.14%\n",
      "Training:Epoch[000/003] Iteration[269/391] Loss: 2.1625 Acc:14.14%\n",
      "Training:Epoch[000/003] Iteration[270/391] Loss: 2.1442 Acc:14.17%\n",
      "Training:Epoch[000/003] Iteration[271/391] Loss: 2.1521 Acc:14.19%\n",
      "Training:Epoch[000/003] Iteration[272/391] Loss: 2.1412 Acc:14.21%\n",
      "Training:Epoch[000/003] Iteration[273/391] Loss: 2.1373 Acc:14.24%\n",
      "Training:Epoch[000/003] Iteration[274/391] Loss: 2.1768 Acc:14.24%\n",
      "Training:Epoch[000/003] Iteration[275/391] Loss: 2.1859 Acc:14.25%\n",
      "Training:Epoch[000/003] Iteration[276/391] Loss: 2.1824 Acc:14.24%\n",
      "Training:Epoch[000/003] Iteration[277/391] Loss: 2.1797 Acc:14.26%\n",
      "Training:Epoch[000/003] Iteration[278/391] Loss: 2.1289 Acc:14.27%\n",
      "Training:Epoch[000/003] Iteration[279/391] Loss: 2.1302 Acc:14.29%\n",
      "Training:Epoch[000/003] Iteration[280/391] Loss: 2.2581 Acc:14.29%\n",
      "Training:Epoch[000/003] Iteration[281/391] Loss: 2.1438 Acc:14.30%\n",
      "Training:Epoch[000/003] Iteration[282/391] Loss: 2.1655 Acc:14.30%\n",
      "Training:Epoch[000/003] Iteration[283/391] Loss: 2.1381 Acc:14.31%\n",
      "Training:Epoch[000/003] Iteration[284/391] Loss: 2.1443 Acc:14.32%\n",
      "Training:Epoch[000/003] Iteration[285/391] Loss: 2.1510 Acc:14.33%\n",
      "Training:Epoch[000/003] Iteration[286/391] Loss: 2.1434 Acc:14.36%\n",
      "Training:Epoch[000/003] Iteration[287/391] Loss: 2.1058 Acc:14.38%\n",
      "Training:Epoch[000/003] Iteration[288/391] Loss: 2.1010 Acc:14.41%\n",
      "Training:Epoch[000/003] Iteration[289/391] Loss: 2.1275 Acc:14.43%\n",
      "Training:Epoch[000/003] Iteration[290/391] Loss: 2.1506 Acc:14.45%\n",
      "Training:Epoch[000/003] Iteration[291/391] Loss: 2.1355 Acc:14.46%\n",
      "Training:Epoch[000/003] Iteration[292/391] Loss: 2.1400 Acc:14.46%\n",
      "Training:Epoch[000/003] Iteration[293/391] Loss: 2.1604 Acc:14.48%\n",
      "Training:Epoch[000/003] Iteration[294/391] Loss: 2.1290 Acc:14.49%\n",
      "Training:Epoch[000/003] Iteration[295/391] Loss: 2.1504 Acc:14.50%\n",
      "Training:Epoch[000/003] Iteration[296/391] Loss: 2.0957 Acc:14.53%\n",
      "Training:Epoch[000/003] Iteration[297/391] Loss: 2.0948 Acc:14.53%\n",
      "Training:Epoch[000/003] Iteration[298/391] Loss: 2.1699 Acc:14.57%\n",
      "Training:Epoch[000/003] Iteration[299/391] Loss: 2.1194 Acc:14.61%\n",
      "Training:Epoch[000/003] Iteration[300/391] Loss: 2.0873 Acc:14.65%\n",
      "Training:Epoch[000/003] Iteration[301/391] Loss: 2.1486 Acc:14.68%\n",
      "Training:Epoch[000/003] Iteration[302/391] Loss: 2.1486 Acc:14.71%\n",
      "Training:Epoch[000/003] Iteration[303/391] Loss: 2.1520 Acc:14.72%\n",
      "Training:Epoch[000/003] Iteration[304/391] Loss: 2.0667 Acc:14.76%\n",
      "Training:Epoch[000/003] Iteration[305/391] Loss: 2.1701 Acc:14.78%\n",
      "Training:Epoch[000/003] Iteration[306/391] Loss: 2.1993 Acc:14.79%\n",
      "Training:Epoch[000/003] Iteration[307/391] Loss: 2.1050 Acc:14.81%\n",
      "Training:Epoch[000/003] Iteration[308/391] Loss: 2.1648 Acc:14.82%\n",
      "Training:Epoch[000/003] Iteration[309/391] Loss: 2.1155 Acc:14.84%\n",
      "Training:Epoch[000/003] Iteration[310/391] Loss: 2.1413 Acc:14.85%\n",
      "Training:Epoch[000/003] Iteration[311/391] Loss: 2.1250 Acc:14.86%\n",
      "Training:Epoch[000/003] Iteration[312/391] Loss: 2.1213 Acc:14.88%\n",
      "Training:Epoch[000/003] Iteration[313/391] Loss: 2.0610 Acc:14.89%\n",
      "Training:Epoch[000/003] Iteration[314/391] Loss: 2.0961 Acc:14.91%\n",
      "Training:Epoch[000/003] Iteration[315/391] Loss: 2.1519 Acc:14.92%\n",
      "Training:Epoch[000/003] Iteration[316/391] Loss: 2.1064 Acc:14.94%\n",
      "Training:Epoch[000/003] Iteration[317/391] Loss: 2.0796 Acc:14.97%\n",
      "Training:Epoch[000/003] Iteration[318/391] Loss: 2.1555 Acc:14.98%\n",
      "Training:Epoch[000/003] Iteration[319/391] Loss: 2.1327 Acc:15.00%\n",
      "Training:Epoch[000/003] Iteration[320/391] Loss: 2.1390 Acc:15.00%\n",
      "Training:Epoch[000/003] Iteration[321/391] Loss: 2.1427 Acc:15.02%\n",
      "Training:Epoch[000/003] Iteration[322/391] Loss: 2.1025 Acc:15.01%\n",
      "Training:Epoch[000/003] Iteration[323/391] Loss: 2.0868 Acc:15.05%\n",
      "Training:Epoch[000/003] Iteration[324/391] Loss: 2.0151 Acc:15.07%\n",
      "Training:Epoch[000/003] Iteration[325/391] Loss: 2.0835 Acc:15.08%\n",
      "Training:Epoch[000/003] Iteration[326/391] Loss: 2.1037 Acc:15.10%\n",
      "Training:Epoch[000/003] Iteration[327/391] Loss: 2.0103 Acc:15.11%\n",
      "Training:Epoch[000/003] Iteration[328/391] Loss: 2.1246 Acc:15.13%\n",
      "Training:Epoch[000/003] Iteration[329/391] Loss: 2.0782 Acc:15.16%\n",
      "Training:Epoch[000/003] Iteration[330/391] Loss: 2.0496 Acc:15.18%\n",
      "Training:Epoch[000/003] Iteration[331/391] Loss: 2.0783 Acc:15.19%\n",
      "Training:Epoch[000/003] Iteration[332/391] Loss: 2.1212 Acc:15.20%\n",
      "Training:Epoch[000/003] Iteration[333/391] Loss: 2.1552 Acc:15.19%\n",
      "Training:Epoch[000/003] Iteration[334/391] Loss: 2.0539 Acc:15.24%\n",
      "Training:Epoch[000/003] Iteration[335/391] Loss: 2.0986 Acc:15.23%\n",
      "Training:Epoch[000/003] Iteration[336/391] Loss: 2.1432 Acc:15.25%\n",
      "Training:Epoch[000/003] Iteration[337/391] Loss: 2.1307 Acc:15.27%\n",
      "Training:Epoch[000/003] Iteration[338/391] Loss: 2.1620 Acc:15.27%\n",
      "Training:Epoch[000/003] Iteration[339/391] Loss: 2.0629 Acc:15.29%\n",
      "Training:Epoch[000/003] Iteration[340/391] Loss: 2.0447 Acc:15.32%\n",
      "Training:Epoch[000/003] Iteration[341/391] Loss: 2.0217 Acc:15.35%\n",
      "Training:Epoch[000/003] Iteration[342/391] Loss: 2.1718 Acc:15.36%\n",
      "Training:Epoch[000/003] Iteration[343/391] Loss: 2.0634 Acc:15.39%\n",
      "Training:Epoch[000/003] Iteration[344/391] Loss: 2.1107 Acc:15.39%\n",
      "Training:Epoch[000/003] Iteration[345/391] Loss: 2.0760 Acc:15.41%\n",
      "Training:Epoch[000/003] Iteration[346/391] Loss: 2.0331 Acc:15.44%\n",
      "Training:Epoch[000/003] Iteration[347/391] Loss: 2.0473 Acc:15.46%\n",
      "Training:Epoch[000/003] Iteration[348/391] Loss: 2.1156 Acc:15.47%\n",
      "Training:Epoch[000/003] Iteration[349/391] Loss: 2.0728 Acc:15.47%\n",
      "Training:Epoch[000/003] Iteration[350/391] Loss: 2.0975 Acc:15.48%\n",
      "Training:Epoch[000/003] Iteration[351/391] Loss: 2.0372 Acc:15.50%\n",
      "Training:Epoch[000/003] Iteration[352/391] Loss: 2.0396 Acc:15.51%\n",
      "Training:Epoch[000/003] Iteration[353/391] Loss: 2.0835 Acc:15.52%\n",
      "Training:Epoch[000/003] Iteration[354/391] Loss: 2.0334 Acc:15.54%\n",
      "Training:Epoch[000/003] Iteration[355/391] Loss: 2.1001 Acc:15.54%\n",
      "Training:Epoch[000/003] Iteration[356/391] Loss: 2.0932 Acc:15.55%\n",
      "Training:Epoch[000/003] Iteration[357/391] Loss: 2.0237 Acc:15.57%\n",
      "Training:Epoch[000/003] Iteration[358/391] Loss: 2.0552 Acc:15.58%\n",
      "Training:Epoch[000/003] Iteration[359/391] Loss: 2.0691 Acc:15.58%\n",
      "Training:Epoch[000/003] Iteration[360/391] Loss: 2.0713 Acc:15.61%\n",
      "Training:Epoch[000/003] Iteration[361/391] Loss: 2.0470 Acc:15.64%\n",
      "Training:Epoch[000/003] Iteration[362/391] Loss: 2.0757 Acc:15.64%\n",
      "Training:Epoch[000/003] Iteration[363/391] Loss: 2.0832 Acc:15.65%\n",
      "Training:Epoch[000/003] Iteration[364/391] Loss: 2.0191 Acc:15.66%\n",
      "Training:Epoch[000/003] Iteration[365/391] Loss: 2.1550 Acc:15.66%\n",
      "Training:Epoch[000/003] Iteration[366/391] Loss: 2.0386 Acc:15.68%\n",
      "Training:Epoch[000/003] Iteration[367/391] Loss: 2.0820 Acc:15.67%\n",
      "Training:Epoch[000/003] Iteration[368/391] Loss: 2.1336 Acc:15.68%\n",
      "Training:Epoch[000/003] Iteration[369/391] Loss: 2.0004 Acc:15.68%\n",
      "Training:Epoch[000/003] Iteration[370/391] Loss: 1.9990 Acc:15.70%\n",
      "Training:Epoch[000/003] Iteration[371/391] Loss: 2.0319 Acc:15.72%\n",
      "Training:Epoch[000/003] Iteration[372/391] Loss: 2.0787 Acc:15.74%\n",
      "Training:Epoch[000/003] Iteration[373/391] Loss: 2.0728 Acc:15.74%\n",
      "Training:Epoch[000/003] Iteration[374/391] Loss: 2.0089 Acc:15.75%\n",
      "Training:Epoch[000/003] Iteration[375/391] Loss: 1.9704 Acc:15.78%\n",
      "Training:Epoch[000/003] Iteration[376/391] Loss: 2.0597 Acc:15.79%\n",
      "Training:Epoch[000/003] Iteration[377/391] Loss: 2.0684 Acc:15.79%\n",
      "Training:Epoch[000/003] Iteration[378/391] Loss: 2.0161 Acc:15.82%\n",
      "Training:Epoch[000/003] Iteration[379/391] Loss: 2.0102 Acc:15.84%\n",
      "Training:Epoch[000/003] Iteration[380/391] Loss: 2.0185 Acc:15.86%\n",
      "Training:Epoch[000/003] Iteration[381/391] Loss: 2.0024 Acc:15.87%\n",
      "Training:Epoch[000/003] Iteration[382/391] Loss: 2.0492 Acc:15.88%\n",
      "Training:Epoch[000/003] Iteration[383/391] Loss: 1.9468 Acc:15.90%\n",
      "Training:Epoch[000/003] Iteration[384/391] Loss: 1.9848 Acc:15.92%\n",
      "Training:Epoch[000/003] Iteration[385/391] Loss: 2.0690 Acc:15.93%\n",
      "Training:Epoch[000/003] Iteration[386/391] Loss: 2.0411 Acc:15.96%\n",
      "Training:Epoch[000/003] Iteration[387/391] Loss: 2.1228 Acc:15.96%\n",
      "Training:Epoch[000/003] Iteration[388/391] Loss: 2.1333 Acc:15.97%\n",
      "Training:Epoch[000/003] Iteration[389/391] Loss: 2.0607 Acc:15.97%\n",
      "Training:Epoch[000/003] Iteration[390/391] Loss: 2.0082 Acc:15.99%\n",
      "Training:Epoch[000/003] Iteration[391/391] Loss: 2.0117 Acc:15.99%\n"
     ]
    }
   ],
   "source": [
    "train_curve = list() \n",
    "valid_curve = list()\n",
    "# 用于记录训练和验证过程中的损失值曲线。\n",
    "# 每个 epoch 中的损失都会被添加到这些列表中。\n",
    "\n",
    "for epoch in range(start_epoch + 1, MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    Mobile_model.train() # 将模型切换到训练模式，启用 dropout 等操作。\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # 遍历训练数据加载器，data 包含一个批次的 inputs（输入图像）和 labels（对应的标签）。\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = Mobile_model(inputs)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        # 清空上一批次的梯度，防止累积。\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        # 通过自动求导计算梯度。\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # 使用优化器根据计算出的梯度更新模型参数。\n",
    "\n",
    "        # 统计分类情况\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # 概率最大值的索引 dim = 1指定按行计算（即对于每个样本，找到所有类别的预测分数中最大的那个）\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).squeeze().cpu().sum().numpy()\n",
    "\n",
    "        # 打印训练信息\n",
    "        loss_mean += loss.item()        # 获取当前批次的损失值。\n",
    "        train_curve.append(loss.item()) # 将当前批次的损失值记录到 train_curve。\n",
    "        if (i+1) % log_interval == 0:\n",
    "            loss_mean = loss_mean / log_interval\n",
    "            print(\"Training:Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, i+1, len(train_loader), loss_mean, correct / total))\n",
    "            loss_mean = 0.\n",
    "\n",
    "    scheduler.step()  # 更新学习率 调用学习率调度器，根据设置调整当前学习率（如按一定步长下降）。\n",
    "\n",
    "    # validate the model\n",
    "    if (epoch+1) % val_interval == 0:\n",
    "\n",
    "        correct_val = 0.\n",
    "        total_val = 0.\n",
    "        loss_val = 0.\n",
    "        Mobile_model.eval() \n",
    "        \n",
    "        # 禁用自动求导，减少内存占用，加速计算。\n",
    "        with torch.no_grad():\n",
    "            for j, data in enumerate(valid_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                bs, ncrops, c, h, w = inputs.size()     # [4, 10, 3, 224, 224]\n",
    "                outputs = Mobile_model(inputs.view(-1, c, h, w))\n",
    "                outputs_avg = outputs.view(bs, ncrops, -1).mean(1)\n",
    "                # 计算 10 个裁剪图像的预测值平均值。\n",
    "\n",
    "                loss = criterion(outputs_avg, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs_avg.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).squeeze().cpu().sum().numpy()\n",
    "\n",
    "                loss_val += loss.item()\n",
    "\n",
    "            loss_val_mean = loss_val/len(valid_loader)\n",
    "            valid_curve.append(loss_val_mean)\n",
    "            print(\"Valid:\\t Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                epoch, MAX_EPOCH, j+1, len(valid_loader), loss_val_mean, correct_val / total_val))\n",
    "        Mobile_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03f339-f941-413a-868f-9b7b90280ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = range(len(train_curve))\n",
    "train_y = train_curve\n",
    "\n",
    "train_iters = len(train_loader)\n",
    "valid_x = np.arange(1, len(valid_curve)+1) * train_iters*val_interval # 由于valid中记录的是epochloss，需要对记录点进行转换到iterations\n",
    "valid_y = valid_curve\n",
    "\n",
    "plt.plot(train_x, train_y, label='Train')\n",
    "plt.plot(valid_x, valid_y, label='Valid')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
